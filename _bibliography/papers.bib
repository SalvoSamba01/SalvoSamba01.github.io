@misc{leotta2026synthforensicsmultigeneratorbenchmarkdetecting,
      title={SynthForensics: A Multi-Generator Benchmark for Detecting Synthetic Video Deepfakes}, 
      author={Roberto Leotta† and Salvatore Alfio Sambataro† and Claudio Vittorio Ragaglia and Mirko Casu and Yuri Petralia and Francesco Guarnera and Luca Guarnera and Sebastiano Battiato},
      journal={arXiv Preprint},
      note={&#8224; Joint first authorship},
      abstract={The landscape of synthetic media has been irrevocably altered by text-to-video (T2V) models, whose outputs are rapidly approaching indistinguishability from reality. Critically, this technology is no longer confined to large-scale labs; the proliferation of efficient, open-source generators is democratizing the ability to create high-fidelity synthetic content on consumer-grade hardware. This makes existing face-centric and manipulation-based benchmarks obsolete. To address this urgent threat, we introduce SynthForensics, to the best of our knowledge the first human-centric benchmark for detecting purely synthetic video deepfakes. The benchmark comprises 6,815 unique videos from five architecturally distinct, state-of-the-art open-source T2V models. Its construction was underpinned by a meticulous two-stage, human-in-the-loop validation to ensure high semantic and visual quality. Each video is provided in four versions (raw, lossless, light, and heavy compression) to enable real-world robustness testing. Experiments demonstrate that state-of-the-art detectors are both fragile and exhibit limited generalization when evaluated on this new domain: we observe a mean performance drop of 29.19\% AUC, with some methods performing worse than random chance, and top models losing over 30 points under heavy compression. The paper further investigates the efficacy of training on SynthForensics as a means to mitigate these observed performance gaps, achieving robust generalization to unseen generators (93.81\% AUC), though at the cost of reduced backward compatibility with traditional manipulation-based deepfakes. The complete dataset and all generation metadata, including the specific prompts and inference parameters for every video, will be made publicly available.},
      pdf={synthforensics.pdf},
      selected={true},
      year={2026},
      eprint={2602.04939},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2602.04939}, 
      preview = {comparison.jpg}
}
